{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cocktail-Party Problem: Attempt 1 of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem statement: Imagine you are in a room where two people are speaking simultaneously. Two microphones have been placed in arbitrary but distinct locations, with respect to the conversation. Each of these microphones records a unique, time-dependent signal, that can be represented as a weighted sum of individual voices from the original conversation. \n",
    "\n",
    "Let $x_{i}(t)$ and $s_{j}(t)$ denote the recorded and source signals, respectively. The subscripts $\\{i,\\,j\\}$ run from $\\{i = 1, 2, ..., n,\\, j=1, 2,..., m\\}$ and represent the number of distinct signals recorded and voices speaking, respectively. We consider the case where $i = j$, but in general, $i \\ne j$. Through our definitions we express the problem as a system of linear equations:  $$x_{i}(t) = a_{ij}s_j(t) + a_{ij+1}s_{j+1}(t) \\\\\n",
    "x_{i+1}(t) = a_{i+1j}s_j(t) + a_{i+1j+1}s_{j+1}(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the $a_{ij}$'s are some parameters that depend on the distances of the microphones from the speakers. Now suppose you wish to estimate the original speech signals $s_j(t)$ and $s_{j+1}(t)$, using only information about the recorded signals $x_i(t)$ and $x_{i+1}(t)$. \n",
    "\n",
    "This is in essence the cocktail-party problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cocktail problem is an example of a more general class of problems called blind source separation (BSS). In general, a BSS problem is concerned with the separation of a set of source signals, from a corresponding set of mixed signals, with little to no information about the source signals or the mixing process. Problems of this type are highly underdetermined, i.e., contain fewer equations than unknown parameters, and typically strong assumptions that depend on the particular problem statement have to be made. Although considered a non-trivial problem, approaches have been developed under a surprising variety of conditions. This draft will focus on the method of independent component analyses (ICA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the time index $t$; in the ICA model, we assume that each mixture $x_j$ as well as for each source signal $s_k$ is a random variable, instead of some proper time signal. Assume we observe $n$ linear mixtures $x_1,\\cdots ,x_n$ of $n$ independent components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_j=a_{j1}s_1+a_{j2}s_2+\\cdots+a_{jn}s_n, \\, \\text{for all} \\, j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed values $x_j(t)$, e.g., the microphone signals in the cocktail party problem, are then a sample of this random variable. Using vector-matrix notation the above mixing model can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\vec{x}= \\textbf{A}\\vec{s} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where henceforth, lowercase arrowed letters will represent vectors and bold-faced capitol letters represent matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ICA model is a generative model, in that it describes how the observed data are generated by a process that mixes the components $s_i$. The mixing matrix and independent components are unknown, all we observe is the random vector $\\vec{x}$, and we must estiamte both $\\textbf{A}$ and $\\vec{s}$ using assumptions as general as possible. After estimating the matrix $\\textbf{A}$, one can compute its inverse $\\textbf{W} = \\textbf{A}^{-1} $, and obtain the independent components by "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\vec{s}= \\textbf{W}\\vec{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting assumption for ICA is that the components $s_i$ are $\\textit{statistically independent}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, any two random scalar-valued variables are said to be independent if information on the value of one does not give any information on the value of the other. Formally speaking, independence can be defined using probability density functons. Let $p(y_1,y_2)$ be the joint probability density function for two random variables $y_1$ and $y_2$. Futhermore, denote $p_i(y_i)$ as the marginal probability density function of either $y_1$ or $y_2$ alone, given as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  p_1(y_1) = \\int p(y_1,y_2)dy_2 \\\\ $$\n",
    "\n",
    "$$ p_2(y_2) = \\int p(y_1,y_2)dy_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random variables $y_1$ and $y_2$ are said to be independent if and only if their joint probability density function is factorizable, as shown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_1,y_2) = p_1(y_1)p_2(y_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above definition we can derive a fundamental property of independent random variables. Given two functions, $h_1$ and $h_2$, we write the joint ecpectation value as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E\\{h_1(y_1)h_2(y_2)\\}= \\int\\int h_1(y_1)h_2(y_2)p(y_1,y_2)dy_1dy_2 \\\\ $$\n",
    "$$ = \\int\\int h_1(y_1)p_1(y_1)h_2(y_2)p_2(y_2)dy_1dy_2 = \\int h_1(y_1)p_1(y_1)dy_1 \\int h_2(y_2)p_2(y_2)dy_2 \\\\$$\n",
    "$$ = E\\{h_1(y_1)\\} E\\{h_2(y_2)\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weaker but useful form of inependence is uncorrelatedness. Two random variables are said to be uncorrelated if their covariance is zero:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E\\{y_1y_2\\}-E\\{y_1\\}E\\{y_2\\} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variables $y_1$ and $y_2$ are independent, they are uncorrelated, however, uncorrelated variables do not imply independence. However, since independence does imply correlation, many ICA methods constrain the estimation procedure so that it always gives uncorrelated estimates of independent components, reducing the number of free parameters and simpifying the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of ICA estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most fundamental restriction on ICA is that the independent components must be non-Gaussian. More rigorously, the distribution for all Gaussian variables related by an orthogonal transformation is the same. In other words, the matrix $\\textbf{A}$ is not identifable for independent Gaussian components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how this \"Gaussianity\" influences the framework of the ICA model we turn to a result from probability theory. The Central Limit Theorem states that the distribution for a sum of independent random variables will become a Gaussian as the number of random variables in the system approaches infinity. As a consequence, the probability density function for a sum of just two independent random variables, is typically more Gaussian than the probability density functions for each of the independent random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminding ourselves of the problem of estimating one of the independent components of the signal data, the vector $\\vec{x}$ can be written as a linear combination of $x_i$, such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\vec{w}^T\\vec{x} = \\sum_iw_ix_i $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\vec{w}$ is a undetermined vector.\n",
    "If we want $\\vec{w}$ to equal one of the independent components, it needs to be one of the rows of $\\textbf{A}^{-1}$. With a change of variables $\\vec{z}=\\textbf{A}^T\\vec{w}$ we have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\vec{w}^T\\vec{x} = \\vec{w}^T\\textbf{A}\\vec{s} = \\vec{z}^T\\vec{s} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that $y$ is a linear combination of $s_i$ with weights given by $z_i$. From the Central Limit Theorem we know that $\\vec{z}^T\\vec{s}$ is more Gaussian than any of the $s_i$ and becomes least Gaussian when it equals one of the $s_i$. This measure of how \"Gaussian\" a distribution is can be used to estimate one of the independent components. In essence, we search for a vector $\\vec{w}$ that $ \\textit{maximizes the non-Gaussianaity}$ of $\\vec{w}^T\\vec{x}$. This $\\vec{w}$ vector corresponds to a  $\\vec{z}$ with a single non-zero component, implying that, $\\vec{w}^T\\vec{x} = \\vec{z}^T \\vec{s} $ equals one of the independent components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures of non-Gaussianaity: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use non-Gaussiananity as a means of ICA estimation we need a way to quantitatively measure the non-Gaussiananity of a random variable $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the classic measures of non-Gaussianaity is the kurtosis function or fourth-order cumulant. The kurtosis of a random variable $y$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ kurt(y) = E\\{y^4\\} - 3(E\\{y^2\\})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variable with unit variance, the kurtosis simply reduces to a normalized version of the fourth moment $E\\{y^4\\}-3$. If $y$ is a Gaussian distributed variable then the fourth order moment equals $$ E\\{y^4\\} = 3(E\\{y^2\\})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the kurtosis is zero. In general, the kurtosis is non-zero for most non-Gaussian random variables. Kurtosis, or rather its absolute value, has been widely used as a measure of non-Gaussianaity in ICA and related fields due to its simplicity, both computational and theoretical. For example, computationally kurtosis is estimated simply by using the fourth moment of the data sample. While theoretically, the analysis is simplified due to the following property of linearity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ kurt(\\alpha x_1 + \\beta x_2) = \\alpha^4 kurt(x_1)+\\beta^4 kurt(x_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the kurtosis framework, one can maximize the deviation away from a Gaussian distribution to calculate the corresponding unknown components. In practice, one would start from some weight vector $\\vec{w}$, based on the available data sample $\\vec{x}(1),...,\\vec{x}(T)$ of a vector $\\vec{x}$. Compute the direction in which the kurtosis of $y=\\vec{w}^T\\vec{x}$ is growing most strongly if positive, or decaying most strongly if negative. Then use a gradient descent method, or something similar, to find the new vector $\\vec{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, by construction kurtosis can be very sensitive to outliers. Its value may depend on only a few observations in the tail ends of the distribution, many standard deviations away from the average, potentially correspondong to irrelevant or erroneous values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negentropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second relevant measure of non-Gaussianaity is given by negentropy and stems from the concept of differential entropy in information theory. Entropy is the basic concept of information theory and quantifies the amount of information measured during an observation of a random variable. The more \"random\", i.e., unstructured and unpredicatable the variable is, the larger its entropy. More rigorously, the differential entropy $H(y)$ for continuous random vector $\\vec{y}$ with density $p(\\vec{y})$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(y) = - \\int p(y)\\,log(p(y)) \\,dy = \\, \\int f(p(y))dy$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a connection between the entropy of $y$ and of another variable $x$,consider the following $\\textit{invertable}$ transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = f(y)$$\n",
    "\n",
    "and denote by $Jf(x)$ the Jacobian matrix of the function $f$, i.e., the matrix of partial derivatives of $f$ at point $y$. The classic relation between the density $p_y$ of $y$ and the density $p_x$ of $x$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_y(y)= \\dfrac{1}{\\mid det\\,J\\,f(f^{-1}(y))\\mid}p_x(f^{-1}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, expressing the entropy as an expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(y) = -E\\{ log\\,p_y(y)\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E\\{log\\,p_y(y)\\} = E\\Bigg\\{log\\bigg[\\dfrac{p_x(f^{-1}(y)}{\\mid det\\,J\\,f(f^{-1}(y)) \\mid}\\bigg]\\Bigg\\} \\\\ $$ \n",
    "\n",
    "$$=E\\Bigg\\{log\\bigg[\\dfrac{p_x(x)}{\\mid det\\, J\\,f(x)\\mid}\\bigg]\\Bigg\\}\\\\ $$ \n",
    "$$= E\\{log\\, p_x(x)\\}-E\\{log\\, \\mid detJ\\,f(x) \\mid\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we obtain the relation between the entopies as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(y) = H(x)+E\\{log\\,\\mid detJ\\, f(x) \\mid\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the entropy is increased in the transformation by $E\\{log\\mid det\\,J\\,f(x)\\mid\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental result of of information theory is that $\\textit{a Gaussian varibable has the largest entropy among all random variables of equal variance}$. This means that the entropy can be used as a measure of non-Gaussianaity. For example, distributions that are clearly concentrated around certain values, i.e., clustering or \"spiked\" density distribution, have very low entropy. Alternatively, the Gaussian distribution is the most random and least structured of all distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a measure of this non-Gaussianaity that is zero with repsect to Gaussian variables and always non-zero and non-negative any non-Gaussian variable we use a modified version of the differential entropy, called negentropy. The negentropy $J$ is defined as follows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(y) = H(y_{Gauss}) - H(y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y_{Gauss}$ is a random variable from the same covariance matrix of $y$. We can clearly see that the negentropy $J$ is zero if and only if $y$ has a Gaussian distribution. The negentropy is the optimal estimator of non-Gaussianaity when worling with statistical properties however, compuationally it is very expensive. Estimating the negentropy requires a potentially non-parametric estiamte of the probabilty density function of $y$, which we do not know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimization of mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach for ICA estimation, inspired by information theory, is minimization of mutual information. Using the concept of differential entropy, we define the mutual information $I$ between $m$ random scalar variables, $y_i\\,\\,i = 1,...,m$ as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(y_1,y_2,...,y_m) = \\sum_{i=1}^{m}H(y_i)-H(y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition of mutual information can be interpreted as a distance using what is called the Kullback-Leibler divergence. This is defined between two n-dimensional probability functions $p^1$ and $p^2$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta (p^1,p^2) = \\int p^1(\\eta)\\,log\\dfrac{p^1(\\eta)}{p^2(\\eta)}d\\eta$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the Kullback-Leibler divergence is always non-negative and zero if and only if the two distributions are equal. This is a direct consequence of the convexity of the negative logarithm, and the application of the Jensen's inequality. The inequality states that for any stictly  convex function $f$ and any random variable $y$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E\\{f(y)\\}\\geq \\,f(E\\{y\\}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the Kullback-Leibler divergence here, let us begin by considering that if random variables $x_i$ were independent, their joint probability density could be factorized according to the definition of independence. Therefore, one might measure the independence of the $x_i$ as the Kullback-Leibler divergence between the real density $p^1=p_x(\\eta)$ and the factorized density $p^2 = p_1(\\eta_1)p_2(\\eta_2)...p_n(\\eta_n)$,, where $p_i$ are the marginal densities of the $x_i$. The interpretation of the Kullback-Liebler divergence in this way implies the following property:  $\\textit{Mutual information is always non-negative, and it is zero if and only if the variables are independent}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " An important property of mutual information  is that we have for an invertable linear transformation $y=Wx$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(y_1,y_2,...,y_n) = \\sum_iH(y_i)-H(x)-log\\mid detW\\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us consider what happens if we constrain the $y_i$ to be $\\textit{uncorrelated}$ and of unit variance. This means that $E\\{yy^T\\}=WE\\{xx^T\\}W^T=I$, which implies $det I = 1 = (detWE\\{xx^T\\}W^T)=(detW)(detE\\{xx^T\\})(detW^T)$ stating that $detW$ must be a constant. Futhermore, for a $y_i$ of unit variance, entropy and negentropy differ only by a constant, and a sign. Thus we obtain, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I(y_1,...y_n)=C-\\sum_iJ(y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $C$ is a constant that does not depend on $W$. The above shows the fundamental relationship between negentropy and mutual information. Since mutual information is the natural information measure of the independence of random variables, we define the ICA of a random vector $x$ as an invertable transformation where the matrix $W$ is determined such that the mutual information of the transformed components $s_i$ is minimized. Finding an invertable transformation $W$ that minimizes the mutual information is roughly equivalent to $\\textit{finding the directions in which the negentropy is maxamized}$. More precisely, it can roughly be thought of as finding the 1-D subspace such that the projection in those subspaces have maximum negentropy. All things considered, the expression above shows that ICA estimation by minimization of mutual information is equivalent to maximizing the sum of non-Gaussianaities of the estimates, when $\\textit{the estimates are constrained to be uncorrelated}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimation: Infomax principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very popular approach for estiamting the ICA model is the maximum likelihood estimation, which is closely related to the infomax principle. Here we discuss this approach, and show that it is essentially equivalent to minimization of mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to formulate directly the likelihood in the noise-free ICA model, and then estimate the real model by a maximum likelihood method. This produces the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoting by $W$ the matrix $A^{-1}$ the log-likelihood takes the form \n",
    "\n",
    "$$L = \\sum_{t=1}^{T}\\sum_{i=1}^{n}log \\,f_i(w_i^Tx(t))+T log\\mid detW\\mid $$   \n",
    "\n",
    "where the $f_i$ are the density functions of the $s_i$ and the $x(t)$ give the various quantities for $x$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conceptually similar contrast function, derived from a neural network viewpoint, focuses on maximizing the output entropy (or information flow) of a neural network with non-linear outputs. Assume that $x$ is the input to the neural network whose outputs take the form $g_i(w_i^Tx)$, where the $g_i$ are some non-linear scalar functions and the $w_i$ are the weight vectors of the neurons. One then wants to maximize the entropy of the outputs:\n",
    "\n",
    "$$L_2 = H(g_1(w_1^Tx),...,g_n(w_n^Tx))$$ \n",
    "\n",
    "If the $g_i$ are well chosen, the princple of network entropy maximization or \"infomax\", is equivalent to maximum likelihood estimation. This equivalence requires that the non-linearities $g_i$ used in the neural network are chosen as a cumulative distribution fucntions that correspond to the densities $f_i$, i.e., $g_i'=f_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An illustration of the connection between likelihood and mutual information is seen by considering the expectation of the log likelihood:\n",
    "\n",
    "$$\\dfrac{1}{T}E\\{L\\}=\\sum_{i=1}^{n}E\\{log \\,f_i(w_i^Tx)\\}+log\\mid detW \\mid $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that if the $f_i$ are equal to the distributions of $w_i^Tx$, the first term would be equal to $-\\sum_iH(w_i^Tx)$ and the likelihood would be equal to the negative of the mutual information, up to an additive constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we have discussed some of the statistical principles underlying the ICA methods. However, before translating these principles into algorithms, it will behoove us to do some preprocessing of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and whitening "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic and necessary preprocessing is to center $x$, i.e., subtract its mean vector $m=E\\{x\\}$ so as to make $x$ a zero-mean variable. This implies that $s$ is a zero-mean variable as well. This procedure is solely to simplify the ICA algorithm can be reversed, without loss of generality, at the end. After estimating the mixing matrix $A$ with centered data, we add the mean vector of $s$ back to the cetered estimates of $s$, given as $A^{-1}m$, where $m$ is the subtracted mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful preprocessing strategy in ICA is whitening. This is done after centering and before the ICA algorithm. Conceptually speaking, whitening describes applying a linear transformation to the observed vector $x$ to obtain a new vector $x'$, such that its components are uncorrelated and their variances equal unity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out PCA, the cousin method of ICA, is aptly suited for this required procedure. As a consequence, a brief introduction of PCA will be given before it is applied to whiteing in our context of ICA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{PCA by variance maximization}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical terms, consider a linear combination of the elements $x_1,...,x_n$ of the vector $x$. \n",
    "\n",
    "$$y_1 = \\sum_{k=1}^{n}w_{k1}x_k=w_1^Tx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $w_{11},...,w_{n1}$ are scalar coefficients or weights, elements of an n-dimensional vector $w_1$, and $w_1^T$ denotes the transpose of $w_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor $y_1$ is called the first principle component of $x$ if the variance of $y_1$ is maximally large. Because the variance depends on both the norm and orientation of the weight vector $w_1$, we impose the constraint that the norm of $w_1$ is constant, in practice equal to 1. Therefore, we look for a weight vector $w_1$ that maximizes our PCA citeria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J_1^{PCA}(w_1)=E\\{y_1^2\\}=E\\{(w_1^Tx)^2\\}$$\n",
    "$$=w_1^TE\\{xx^T\\}w_1=w_1^TC_xw_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that $\\mid\\mid w_1 \\mid\\mid=1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $E\\{\\cdots\\}$ is the expectation over the (unknown) density of input vector $x$, and the norm of $w_1$ is the usual Euclidean norm given as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mid\\mid w_1\\mid\\mid =(w_1^Tw_1)^{\\dfrac{1}{2}}=\\Bigg[\\sum_{k=1}^{n}w_{k1}^2 \\Bigg]^{\\dfrac{1}{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $C_x$ is the $n\\times n$ covariance matrix of $x$ given for the zero-mean vector $x$ by the correlation matrix\n",
    "\n",
    "$$C_x = E\\{xx^T\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the PCA problem is given in terms of the unit-length eigenvectors $e_1,...,e_n$ of the matrix $C_x$. The ordering of the eigenvenctors is such that the corresponding eigenvalues $d_1,...,d_n$ satisfy $d_1\\geq d_2\\geq \\cdots \\geq d_n$. The solution is given by $$w_1 = e_1 $$\n",
    "\n",
    "Thus, the first principle component of $x$ is $y_1 = e_1^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The citerion for $J_1^{PCA}$ can be generalized to $m$ principle components, with $m$ being any number between 1 and $n$. The $m-th$ principle component is given by $y_m = w_m^T$ with $w_m$ the corresponding unit norm weight vector. The variance of $y_m$ is now maximized under the constraint that $y_m$ is uncorrelated with all the previously found principle components\n",
    "\n",
    "$$E\\{y_my_k\\} = 0, k<m $$\n",
    "\n",
    "$$ E\\{y_my_k\\} = E\\{(w_m^Tx)(w_k^Tx)\\}=w_m^TC_xw_k=0$$\n",
    "\n",
    "For the second principle component, we have the condition that\n",
    "\n",
    "$$w^T_2Cw_1=d_1w_2^Te_1=0 $$\n",
    "\n",
    "because we already know that $w1=e2$ we are looking for the maximal variance $E\\{y_2^2\\}=E\\{(w_2^Tx)^2\\}$in the subspace orthogonal to the first eigenvector of $C_x$. The solution is given by \n",
    "\n",
    "$$w_2=e_2 $$\n",
    "\n",
    "and from recursive iteration it follows that\n",
    "\n",
    "$$w_k=e_k $$\n",
    "\n",
    "Thus the kth principle component is $y_k=e_k^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to whitening. As already mentioned, the ICA problem is greatly simplified if the observed mixture vectors are first whitened. A zero-mean random vector $z=(z_1...z_n)^T$ is said to be white if its elements $z_i$ are uncorrelated and have unit variances $$E\\{z_iz_j\\}=\\delta_{ij} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because whitening is essentially decorrelation followed by scaling, PCA allows whitening to be done with a linear operation. In the PCA context, the problem of whitening becomes: Given a random vector $x$ with $n$ elements, find a linear transformation $V$ into another vector $z$ such that\n",
    "$$z=Vx $$ is white.\n",
    "\n",
    "The problem has a straightforward solution in terms of the PCA expansion. Let $E = (e_1...e_n)$ be the matrix whose columns are the unit-norm eigenvectors of the covariance matrix $C_x=E\\{xx^T\\}$. These can be computed from a sample of the vectors $x$. Let $D=diag(d_1...d_n)$ be the diagonal matrix of the eigenvalues of $C$. Then a linear whitening transform is given by\n",
    "\n",
    "$$V=D^{-1/2}E^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix always exists when the eigenvalues $d_i$ are positive. To show that $V$ is a whitening transformation recall that $C_x$ can be written in terms of its eigenvectors and eigenvalue matrices $E$ and $D$ as $C_x = EDE^T$. With $E$ an orthogonal matrix satisfying $E^TE=EE^T=I$ it holds,\n",
    "$$E\\{zz^T\\}=VE\\{xx^T\\}V^T=D^{-1/2}E^TEDE^TED^{-1/2}=I $$ which says that the covariance of $z$ is the unit matrix and hence, $z$ is white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear operator $V$ is not unique, any matrix $UV$ with $U$ being an orthonormal matrix, is also a whitening matrix. An important example is the matrix \n",
    "$$ED^{-1/2}E^T $$ called the inverse square root of $C_x$, and written as $C_x^{-1/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
