{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Nets\n",
    "\n",
    "An image of a recurrent neural network looks like this:\n",
    "\n",
    "![Image of a Recurrent Network](http://www.mattmoocar.me/img/portfolio/rnn.gif)\n",
    "\n",
    "The difference between this and and a normal neural network is that some of the outputs refer back layer point back to themselves or to previous layers, that is, there are loops.  If you are like me, you will probably naively think that we are making the layers more complex, but what is actually going on is that recurrent networks are designed for modeling time series data.  And the backwards layers, represent information which is passed into the calculation of the next time step.\n",
    "\n",
    "Mathematically, our data might be written as\n",
    "\\begin{equation}\n",
    "w\\cdot x^i + \\tilde w\\cdot y^{i-1}\n",
    "\\end{equation}\n",
    "\n",
    "where $w$ is the vector containing the weights associated wtih the current inputs $x^i$ and $\\tilde w$ is the vector containing the weights of the outputs of the previous time steps $y^{i-1}$.  I am getting a little bit sloppy with the terminology, the outputs don't necessary have to be from the output layer.\n",
    "\n",
    "If this is giving you deja vu because it looks like numerical simulations of partial differential equations and ordinary differential equations, then you are not alone.  Tensorflow has a tutorial for solving Laplace's wave equation using a recurrent convolution network (of course, in this case, we know the weights): https://www.tensorflow.org/tutorials/pdes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Long Short Term Memory\n",
    "\n",
    "Recurrent networks success has a lot to do with an architecture referred to as LSTM or long short term memory.  This is a special architecture which exists to pass information through multiple time steps.\n",
    "\n",
    "![Image of LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "The first long memory storage in networks.\n",
    "\n",
    "![Image](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)\n",
    "\n",
    "This network has a couple of parts to it.  At the top, we have a way to pass information onto the next step.\n",
    "\n",
    "![Image](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)\n",
    "\n",
    "Then we have a neural network which decides which information to keep\n",
    "\n",
    "![Gate](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "\n",
    "We have a neural network which potentially adds information into the memory\n",
    "![Add information](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "\n",
    "Finally we have the neural network which predicts the new output\n",
    "![Prediction](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
    "\n",
    "\n",
    "There are a number of variants on the LSTM, details which can be read at http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (also the source of the above pictures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Time\n",
    "A \"toy\"example. Our data consists of an input sequence and an output sequence described as follows.\n",
    "\n",
    "Input Sequence: At time step t, $X_t$ has a 50% chance of being 1 (and a 50% chance of being 0). E.g., X might be [1, 0, 0, 1, 1, 1 â€¦ ].\n",
    "Output sequence (Y): At time step t, $X_t = X_t$ unless $X_{t-3}$ =1 in which case X is 1 or $X_{t-8}$ = 1 in which case Y_t = 0. In the case where  $X_{t-3} =1$ and  $X_{t-8} =1$ then $Y_t = X_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate our sequences:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = X\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            Y[i] = 1\n",
    "        elif X[i-8] == 1:\n",
    "            Y[i] = 0\n",
    "        if  X[i-3] == 1 & X[i-8]==1:\n",
    "            Y[i] = X[i]\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L95\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "sess = tf.Session()\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    training_losses = []\n",
    "    for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "        training_loss = 0\n",
    "        training_state = np.zeros((batch_size, state_size))\n",
    "        if verbose:\n",
    "            print(\"\\nEPOCH\", idx)\n",
    "        for step, (X, Y) in enumerate(epoch):\n",
    "            tr_losses, training_loss_, training_state, _ = \\\n",
    "                sess.run([losses,\n",
    "                          total_loss,\n",
    "                          final_state,\n",
    "                          train_step],\n",
    "                              feed_dict={x:X, y:Y, init_state:training_state})\n",
    "            training_loss += training_loss_\n",
    "            if step % 100 == 0 and step > 0:\n",
    "                if verbose:\n",
    "                    print(\"Average loss at step\", step,\n",
    "                          \"for last 250 steps:\", training_loss/100)\n",
    "                training_losses.append(training_loss/100)\n",
    "                training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 250 steps: 0.04310020060278475\n",
      "Average loss at step 200 for last 250 steps: 0.004288288420066237\n",
      "Average loss at step 300 for last 250 steps: 0.0025268865446560086\n",
      "Average loss at step 400 for last 250 steps: 0.0017965291009750218\n",
      "Average loss at step 500 for last 250 steps: 0.0013944605318829417\n",
      "Average loss at step 600 for last 250 steps: 0.0011395693523809313\n",
      "Average loss at step 700 for last 250 steps: 0.0009634618635755032\n",
      "Average loss at step 800 for last 250 steps: 0.000834473121794872\n",
      "Average loss at step 900 for last 250 steps: 0.0007359223498497158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f77bcc931d0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77c1f9e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(1,num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ],\n",
       "       [-0.8135929 , -0.8256537 , -0.91269255, -0.9634326 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(rnn_outputs, feed_dict={x:np.zeros((200,5))})[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bibliography: Code: https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n",
    "sources: http://colah.github.io/posts/2015-08-Understanding-LSTMs/, www.deeplearningbook.org, Hands on Machine Learning with Scikit-Learn and Tensorflow by Aurelien Geron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
